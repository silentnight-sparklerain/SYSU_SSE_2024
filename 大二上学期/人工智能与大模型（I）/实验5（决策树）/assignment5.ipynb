{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color = black size=6>实验五:决策树</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color = blue size=4>第一部分:函数介绍</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)Counter类的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({133: 2, 132: 1})\n",
      "0\n",
      "2\n",
      "dict_values([2, 1])\n",
      "[133, 132]\n"
     ]
    }
   ],
   "source": [
    "x=np.array([[0,133,1],[0,132,0],[0,133,0]])\n",
    "#使用Counter类对数组第2列进行遍历\n",
    "counter = Counter(x[:,1])\n",
    "#第2列中有1个132和2个133，输出该counter对象可以统计这列的数值情况，便于之后的统计\n",
    "print(counter)\n",
    "#因为第2列中没有为0的值，所以返回0\n",
    "print(counter[0])\n",
    "#因为第2列中有2个133，所以返回2\n",
    "print(counter[133])\n",
    "#一般的字典操作方法都能在该类中使用，例如可以通过values函数返回该列的非重复值的个数，方便对某列的非重复值的个数进行查看\n",
    "print(counter.values())\n",
    "#可以输出所有非重复值\n",
    "print(list(counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)使用numpy中的unique实现非重复值的提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1 132 133] [132 133]\n"
     ]
    }
   ],
   "source": [
    "x=np.array([[0,133,1],[0,132,0],[0,133,0]])\n",
    "# 提取整个x矩阵中的非重复值\n",
    "a=np.unique(x[:])\n",
    "# 提取x矩阵中第2列的非重复值\n",
    "a1=np.unique(x[:,1])\n",
    "print(a,a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color = blue size=4>第二部分:实验任务</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color = blue size=3>1) 任务一</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任务一要求完成不同量化标准(信息增益, 信息增益率, 基尼系数)下的结点划分函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">该数据集(train_titanic.csv)为分类数据集，为泰坦尼克号的部分乘客信息以及最后是否生还。包括了四个属性特征以及一个标签(即为Survived,代表是否生还),属性特征分别为Sex(性别)，sibsp(堂兄弟妹个数)，Parch(父母与小孩的个数)，Pclass(乘客等级)  \n",
    "其中该数据集无缺失值和异常值，且所有连续变量已自动转换为离散变量,标签(Survived)也自动转变为离散变量0和1，所以你无需进行数据预处理，可以直接使用该数据集。</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">1) 使用pandas库将训练数据集'train_titanic.csv'载入到Dataframe对象中</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 3]\n",
      " [1 1 0 1]\n",
      " [1 0 0 3]\n",
      " ...\n",
      " [0 0 0 3]\n",
      " [0 0 0 3]\n",
      " [0 1 1 3]]\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "#----your code here\n",
    "train_frame = pd.read_csv('train_titanic.csv')\n",
    "\n",
    "train_feature=train_frame\n",
    "train_outcome=train_frame[['Survived']]\n",
    "\n",
    "train_f = np.array(train_feature)\n",
    "train_o = np.array(train_outcome)\n",
    "\n",
    "train_f = train_f[:,0:4]\n",
    "# 留意特征数组和标签数组的形状\n",
    "print(train_f)\n",
    "print(train_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">2) 编写函数，给定任何标签数组计算其信息熵  \n",
    "    输入：标签数组  \n",
    "    输出：该数组对应的信息熵  \n",
    "    计算信息熵公式:\n",
    "某数组包含K个不同的取值，样本为第k(k=1,2,...,K)个值的数量所占比例为p_k,则其信息熵为$$Ent=-\\sum_{k=1}^K p_k log_2 p_k$$</span>\n",
    "    \n",
    "    \n",
    "<span style=\"color:purple\">例:  \n",
    "    输入:[[0],[1]]   \n",
    "    输出:(-$\\frac{1}{2}$ $log_2$ $\\frac{1}{2}$)+(-$\\frac{1}{2}$ $log_2$ $\\frac{1}{2}$)=1</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面是计算信息熵的函数\n",
    "# 其输入是数据集的标签数组，输出是该标签数组的信息熵\n",
    "# 请补全这个函数的代码，计算信息熵\n",
    "def entropy(label):\n",
    "    # 如果传入的 label 是 Python 的 set（无序且不可按索引访问），\n",
    "    # 先将其转换为 list 再用 np.array 转为 NumPy 数组，以便后续能使用 NumPy 的索引、切片或按维度处理。\n",
    "    # 这样可以保证后续对 label 的计数/统计操作（如 Counter 或按列取值）正常工作。\n",
    "    if isinstance(label, set):\n",
    "        label=np.array(list(label))\n",
    "    if label.ndim==1:\n",
    "        counter = Counter(label)\n",
    "    else:\n",
    "        counter = Counter(label[:,0])\n",
    "    ent = 0.0\n",
    "    # 请补全下面的代码，计算信息熵\n",
    "    # -------Your Code Here-------\n",
    "    # Tips: 遍历counter.values()，计算每个类别的概率，然后计算信息熵\n",
    "    for  in :\n",
    "        # Tips: 计算标签数组中每个标签出现的概率\n",
    "        p = \n",
    "        # Tips: 计算信息熵-p*log2(p)，并累加到ent变量中。可使用math.log2函数计算以2为底的对数\n",
    "        ent += \n",
    "    return ent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">3) 编写函数，函数功能为将所给的数据集按照指定维度的特征进行划分为若干个不同的数据集  \n",
    "    【输入】：特征集合，标签集合，指定维度  \n",
    "    【输出】：划分后所得到的子数据集特征集合，子数据集标签集合</span>\n",
    "\n",
    "<span style=\"color:purple\">例子:  \n",
    "    【输入】:特征集合:[[0,0,0],[0,0,1],[1,0,2]]  \n",
    "    标签集合:[[0],[1],[2]]  \n",
    "    指定维度:0</span>  \n",
    "    \n",
    "<span style=\"color:purple\">【输出】:[[0,0,0],[0,0,1]]和[[1,0,2]]  \n",
    "    [[0],[1]]和[[2]]  \n",
    "    tips:即将特征按其第0维度进行划分，由于第0维特征有0和1两个不同的数值，所以特征集合划分为[[0,0,0],[0,0,1]]和[[1,0,2]]，标签集合划分为[[0],[1]]和[[2]]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们给出一个划分数据集的函数split，请对这个函数进行填空，补全其实现\n",
    "# 该函数的输入是数据集的特征矩阵feature，标签数组label，以及划分的维度d\n",
    "# 该函数的输出是划分后的特征子集列表split_feature和标签子集列表split_label\n",
    "# 例如，假设feature是一个形状为(n_samples, n_features)的NumPy数组，label是一个形状为(n_samples,)的NumPy数组\n",
    "# 则split(feature, label, d)会返回两个列表：\n",
    "# split_feature = [feature_subset_1, feature_subset_2, ..., feature_subset_k]\n",
    "# split_label = [label_subset_1, label_subset_2, ..., label_subset_k]\n",
    "# 其中k是特征的第d列中唯一值的个数，feature_subset_i是feature中第d列等于某个唯一值的所有样本构成的子集，label_subset_i是对应的标签子集\n",
    "\n",
    "# 请补全下面的代码，计算信息熵\n",
    "def split(feature, label, d):\n",
    "    # 使用 np.unique() 函数获取特征矩阵feature的第 d 列中的唯一值列表 unique_vals\n",
    "    # -------Your Code Here-------\n",
    "    unique_vals=\n",
    "    if isinstance(label, set):\n",
    "        label=np.array(list(label))\n",
    "    split_feature=[]\n",
    "    split_label=[]\n",
    "    for i in range(len(unique_vals)):\n",
    "        #    使用表达式 (feature[:,d] == unique_vals[i]) 获取特征矩阵中第 d 列等于 unique_vals[i] 的布尔索引 indices。\n",
    "        #    将满足条件的特征子集 feature[indices] 添加到 split_feature 列表。\n",
    "        #    将对应的标签子集 label[indices] 添加到 split_label 列表。\n",
    "        # -------Your Code Here-------\n",
    "        indices=\n",
    "        split_feature.append()\n",
    "        split_label.append()\n",
    "    return split_feature,split_label\n",
    "\n",
    "# Tips:此处会用到的函数\n",
    "# 1.np.unique()\n",
    "# ---------------------\n",
    "# np.unique() 是 NumPy 库中的一个函数，用于提取数组中所有不重复的元素，并返回一个按照升序排序的一维数组。\n",
    "# 它常用于数据去重、统计数组中不同元素的个数或寻找数据集中出现的所有唯一值。\n",
    "#\n",
    "# 示例：\n",
    "#   import numpy as np\n",
    "#   arr = np.array([4, 2, 2, 8, 4, 6])\n",
    "#   unique_arr = np.unique(arr)\n",
    "#   # unique_arr 的输出为: array([2, 4, 6, 8])\n",
    "#\n",
    "#\n",
    "# 2. Python 布尔索引：\n",
    "# -------------------------\n",
    "# 布尔索引允许我们通过一个布尔（True/False）数组来从原始数组中筛选出满足特定条件的元素，\n",
    "# 而不需要显式地写循环。条件表达式会返回一个与原始数组形状相同的布尔数组，然后将该布尔数组作为索引，\n",
    "# 得到所有对应位置为 True 的元素。\n",
    "#\n",
    "# 示例：\n",
    "#   import numpy as np\n",
    "#   arr = np.array([10, 15, 20, 25, 30])\n",
    "#   condition = arr > 20   # 得到布尔数组: array([False, False, False, True, True])\n",
    "#   filtered_arr = arr[condition]\n",
    "#   # filtered_arr 的输出为: array([25, 30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">4) 编写函数，遍历找出该特征集合中信息增益(使用ID3算法中的公式计算)【最大】的特征  \n",
    "    输入：特征集合，标签集合  \n",
    "    输出：该次划分的最佳信息增益值，最佳划分维度  \n",
    "    计算信息增益公式:  \n",
    "    某数据集D有若干特征值以及对应的标签值，其总样本大小为|D|,这里取其中一个特征feature,该特征包含V个不同的取值，其中值为第v(v=1,2,...,V)个值的样本数量为|$D^v$|，$(此处有\\sum_{v=1}^VD^v=|D|)$,则该特征值对应的信息增益为$$Gain(D,feature)=Ent(D)-\\sum_{v=1}^K \\frac{|D^v|}{D} Ent(D^v)$$\n",
    "该函数中你需要计算出每个特征的信息增益，然后返回最佳的信息增益值和对应的特征的维数</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 请按照下面的实现思路，补全find_opt_feature_gain的代码，计算信息增益\n",
    "# 实现思路：\n",
    "# 1. 首先计算整体标签 y_label 的熵 Ent。\n",
    "# 2. 对于每个特征列（即每个维度 d）：\n",
    "#     a. 调用 split() 函数，将数据按照该特征的不同取值进行划分，\n",
    "#         得到子特征集合 feature 和对应的子标签集合 label。\n",
    "#     b. 对每个划分好的子数据集，计算该子数据集的熵，并根据该子集在原始数据中的比例加权，\n",
    "#         得到该特征划分后的加权熵总和。\n",
    "#     c. 用整体熵 Ent 减去加权熵，就得到了该特征的“信息增益”。\n",
    "# 3. 比较所有特征的信息增益，选择信息增益最大的特征作为最佳划分特征。\n",
    "\n",
    "# Tips: 可能会用到的函数和语法：\n",
    "#     - entropy(): 前面实现的函数，计算给定标签集的熵值。\n",
    "#     - split(): 前面实现的函数，按照指定特征对数据进行划分，生成不同取值对应的数据子集。\n",
    "\n",
    "\n",
    "def find_opt_feature_gain(x_data, y_label):\n",
    "    best_gain = -float('inf')\n",
    "    best_dimension = -1\n",
    "    # 使用前面实现的entropy()函数计算整体标签 y_label 的熵 Ent。\n",
    "    # ----- Your Code Here-----\n",
    "    Ent=\n",
    "    \n",
    "    for d in range(x_data.shape[1]):\n",
    "        # 使用前面实现的split函数将数据集按照维度d进行划分\n",
    "        # ----- Your Code Here-----\n",
    "        feature,label= \n",
    "        gain,ent = 0, 0\n",
    "        # 遍历每个划分好的子数据集，并计算该子数据集的加权熵\n",
    "        for i in range(len(feature)):\n",
    "            # 计算该子数据集的加权熵\n",
    "            # 权重|Dv|/|D|为feature[i]的大小除以x_data的大小\n",
    "            # 使用entropy()函数计算label[i]的信息熵\n",
    "            # ----- Your Code Here ------\n",
    "            ent+=\n",
    "        gain=Ent-ent\n",
    "        if gain > best_gain:\n",
    "            # 记录最优的信息增益和此时的划分维度\n",
    "            # ------ Your Code Here ------\n",
    "            best_gain = \n",
    "            best_dimension = \n",
    "                \n",
    "    return best_gain, best_dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">5) 编写函数，遍历找出该特征集合中信息增益率(使用C4.5算法中的公式计算)【最大】的特征  \n",
    "    输入：特征集合，标签集合  \n",
    "    输出：最佳划分的信息增益率值，对应的划分维度  \n",
    "    计算信息增益率公式:  \n",
    "    某数据集D有若干特征值以及对应的标签值，其总样本大小为|D|,这里取其中一个特征类型feature,该特征包含V个不同的取值，值为第v(v=1,2,...,V)个值的样本数量为|$D^v$|,该特征列本身的信息熵为Ent(feature),则该特征值对应的信息增益率为$$Gain\\_ratio(D,feature)=\\frac{Gain(D,feature)}{Ent(feature)}$$  \n",
    "该函数中你需要计算每个特征的信息增益率，之后返回最佳的信息增益率和对应的特征维数</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 请按照下面的实现思路，补全find_opt_feature_gainratio的代码，计算信息增益率\n",
    "# 实现思路：\n",
    "# 1. 首先计算整体标签 y_label 的熵 Ent。\n",
    "# 2. 对于每个特征列（即每个维度 d）：\n",
    "#     a. 调用 split() 函数，将数据按照该特征的不同取值进行划分，\n",
    "#         得到子特征集合 feature 和对应的子标签集合 label。\n",
    "#     b. 对每个划分好的子数据集，计算该子数据集的熵，并根据该子集在原始数据中的比例加权，\n",
    "#         得到该特征划分后的加权熵总和。\n",
    "#     c. 用整体熵 Ent 减去加权熵，得到该特征的信息增益。\n",
    "#     d. 计算该特征本身的熵 Ent(feature)。\n",
    "#     e. 用信息增益除以特征熵，得到该特征的信息增益率。\n",
    "# 3. 比较所有特征的信息增益率，选择信息增益率最大的特征作为最佳划分特征。\n",
    "# Tips: 可能会用到的函数和语法：\n",
    "#     - entropy(): 前面实现的函数，计算给定标签集的熵值。\n",
    "#     - split(): 前面实现的函数，按照指定特征对数据进行划分，生成不同取值对应的数据子集。\n",
    "\n",
    "def find_opt_feature_gainratio(x_data, y_label):\n",
    "    best_ratio = -float('inf')\n",
    "    best_dimension = -1\n",
    "    # 使用前面实现的entropy()函数计算整体标签 y_label 的熵 Ent。\n",
    "    # ----- Your Code Here-----\n",
    "    Ent=\n",
    "    for d in range(x_data.shape[1]):\n",
    "        # 使用前面实现的split函数将数据集按照维度d进行划分\n",
    "        # ----- Your Code Here-----\n",
    "        feature,label= \n",
    "        gain,ent = 0, 0\n",
    "        for i in range(len(feature)):\n",
    "            # 计算该子数据集的加权熵\n",
    "            # 权重|Dv|/|D|为feature[i]的大小除以x_data的大小\n",
    "            # 使用entropy()函数计算label[i]的信息熵\n",
    "            # ----- Your Code Here ------\n",
    "            ent+=\n",
    "        # 计算d维特征x_data[:,d]本身的熵，即Ent(feature)\n",
    "        # Tips: 使用前面实现的entropy()函数\n",
    "        # ----- Your Code Here ------\n",
    "        ent_feature=\n",
    "        if ent_feature!=0:\n",
    "            # 计算d维特征的信息增益和信息增益率\n",
    "            # ----- Your Code Here ------\n",
    "            gain=\n",
    "            gain_ratio=\n",
    "            if gain_ratio > best_ratio:\n",
    "                best_ratio = gain_ratio\n",
    "                best_dimension = d\n",
    "    return best_ratio, best_dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">6) 编写函数，遍历找出该特征集合中基尼系数(使用CART算法中的公式计算)最小的特征以及最佳的划分值  \n",
    "    输入：特征集合，标签集合  \n",
    "    输出：最佳的基尼系数，对应的划分维度，最佳划分值  \n",
    "    计算基尼系数公式:  \n",
    "    某数据集D有若干特征值以及对应的标签值，其总样本大小为|D|,该集合中标签类别总共有K类，第k类样本所占比例为$p_k$(k=1,2,..,K),则该数据集对应的基尼系数为$$Gini(D)=1-\\sum_{k=1}^K{p_k}^2$$  \n",
    "    而取其中一个特征feature，选定该特征的一个值value，根据该特征的值是否为value将数据集分为两个子集$D_1$和$D_2$,则该特征对应的基尼系数为$$Gini\\_index(D,feature)=\\frac{|D_1|}{|D|}Gini(D_1)+\\frac{|D_2|}{|D|}Gini(D_2)$$\n",
    "该函数中你需要遍历每一列特征，找出每列中的非重复值，计算出每个非重复值的基尼系数，返回最小的基尼系数、对应的特征维数和非重复值（分类值）。</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tips:可能用到的函数和语法：\n",
    "# - np.unique():\n",
    "#     提取数组中的所有不重复元素，并返回一个排好序的一维数组，\n",
    "#     用于提取某个特征 d 中的非重复值\n",
    "#     例如 np.unique([3, 1, 2, 2]) 得到 [1, 2, 3]。\n",
    "# - 布尔索引:\n",
    "#     (x_data[:, d] == unique_values[i]) 会对 x_data 的第 d 列进行逐元素比较，\n",
    "#     返回一个布尔数组，指明哪些样本的该列值等于 unique_values[i]。使用此布尔数组可以从原数组中直接筛选出符合条件的行。\n",
    "# - Counter:\n",
    "#     用于统计列表或数组中各元素出现的次数，\n",
    "#     例如 Counter([1, 2, 2, 3]) 返回 {1: 1, 2: 2, 3: 1}。\n",
    "#     在这里，Counter 用于统计不同标签在某个子集中的分布情况，\n",
    "#     获得 K 类标签中每一类样本的个数\n",
    "\n",
    "def find_opt_feature_gini(x_data, y_label):\n",
    "    best_gini = float('inf')\n",
    "    best_dimension = -1\n",
    "    #遍历 x_data 中的每个特征 d:\n",
    "    for d in range(x_data.shape[1]):\n",
    "        # 使用np.unique()计算特征 d 的唯一取值集合 unique_values\n",
    "        # -------Your Code Here-------\n",
    "        unique_values=\n",
    "        \n",
    "        #遍历 unique_values 中的每个取值 v:\n",
    "        for i in range(len(unique_values)):\n",
    "            gini_sum=0\n",
    "            # ---- Your Code Here -----\n",
    "            #选取 x_data[:, d] == v 的样本索引\n",
    "            indice_v=(x_data[:,d] == )\n",
    "            #选取 x_data[:, d] != v 的样本索引\n",
    "            indice_nv=(x_data[:,d] != )\n",
    "\n",
    "            y_v, y_nv = y_label[indice_v,0], y_label[indice_nv,0]\n",
    "            n_v, n_nv = len(y_v), len(y_nv)\n",
    "\n",
    "            counter_yv, counter_ynv = Counter(y_v), Counter(y_nv)\n",
    "            # 计算特征 d 取值为 v 的子数据集的基尼系数 gini_v\n",
    "            temp_v = 0\n",
    "            for y_k in counter_yv.values():\n",
    "                # 计算 \\sum_k (p_k)^2，其中 p_k 是标签类别 k 在子数据集中的比例\n",
    "                # Tips: p_k = y_k / n_v，其中 y_k 是类别 k 的样本数，n_v 是子数据集的总样本数\n",
    "                # -------Your Code Here-------\n",
    "                temp_v += \n",
    "            gini_v = 1 - temp_v\n",
    "            \n",
    "            # 如果 n_nv 为0，说明所有样本都落在特征 d 取值为 v 的子数据集中\n",
    "            if n_nv==0:\n",
    "                gini_nv = 0\n",
    "            else:\n",
    "                # 计算特征 d 取值不为 v 的子数据集的基尼系数 gini_nv\n",
    "                temp_nv = 0\n",
    "                for y_k in counter_ynv.values():\n",
    "                    # 计算 \\sum_k (p_k)^2，其中 p_k 是标签类别 k 在子数据集中的比例\n",
    "                    # Tips: p_k = y_k / n_nv，其中 y_k 是类别 k 的样本数，n_nv 是子数据集的总样本数\n",
    "                    # -------Your Code Here-------\n",
    "                    temp_nv += \n",
    "                gini_nv = 1 - temp_nv\n",
    "\n",
    "            # 计算加权基尼系数 gini_sum\n",
    "            # -------Your Code Here-------\n",
    "            gini_sum = \n",
    "            \n",
    "            #如果 gini_sum < best_gini:\n",
    "            #返回best_gini, best_dimension,best_value\n",
    "            if gini_sum<best_gini:\n",
    "                best_gini=gini_sum\n",
    "                best_dimension=d\n",
    "                best_value=unique_values[i]\n",
    "                \n",
    "    return best_gini, best_dimension,best_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">7) 应用之前你在第4、5、6个部分编写的三个函数，在训练数据集'train_titanic.csv'上依次使用这些函数进行【一次】结点划分，并输出对应的最佳特征维数以及相应的信息增益值/信息增益率/(基尼系数和分类值)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳维数: 0 信息增益值: 0.10750711887455178\n",
      "最佳维数: 0 信息增益率: 0.11339165967945304\n",
      "最佳维数: 0 基尼系数: 0.2964915724641573 分类值: 0\n"
     ]
    }
   ],
   "source": [
    "#----your code here\n",
    "best1_entropy, best1_dimension = find_opt_feature_gain(train_f, train_o)\n",
    "print(\"最佳维数:\",best1_dimension,\"信息增益值:\",best1_entropy)\n",
    "data1,label1=split(train_f,train_o,best1_dimension)\n",
    "\n",
    "best2_entropy, best2_dimension = find_opt_feature_gainratio(train_f, train_o)\n",
    "print(\"最佳维数:\",best2_dimension,\"信息增益率:\",best2_entropy)\n",
    "data2,label2=split(train_f,train_o,best2_dimension)\n",
    "\n",
    "best3_entropy, best3_dimension,best3_value= find_opt_feature_gini(train_f, train_o)\n",
    "print(\"最佳维数:\",best3_dimension,\"基尼系数:\",best3_entropy,\"分类值:\",best3_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color = blue size=3>2) 任务二</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任务二承接任务一，用【ID3】算法实现一棵完整的决策树。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">利用任务一中的find_opt_feature_entropy函数，完成DTree类中的TreeGenerate、train函数以完成决策树的构建。并完成DTree类中的predict函数来用构建好的决策树来对测试数据集进行预测并输出预测准确率。</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 树结点类\n",
    "class Node:\n",
    "    def __init__(self, isLeaf=True, label=-1, feature_index=-1):\n",
    "        self.isLeaf = isLeaf # isLeaf表示该结点是否是叶结点\n",
    "        self.label = label # label表示该叶结点的label（当结点为叶结点时有用）\n",
    "        self.feature_index = feature_index # feature_index表示该分支结点的划分特征的序号（当结点为分支结点时有用）\n",
    "        self.children = {} # children表示该结点的所有孩子结点，dict类型，方便进行决策树的搜索\n",
    "        \n",
    "    def addNode(self, val, node):\n",
    "        self.children[val] = node #为当前结点增加一个划分特征的值为val的孩子结点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 决策树类\n",
    "class DTree:\n",
    "    def __init__(self):\n",
    "        self.tree_root = None #决策树的根结点\n",
    "        self.possible_value = {} # 用于存储每个特征可能的取值\n",
    "    \n",
    "    def TreeGenerate(self, D, A):#D为数据集，A为特征集\n",
    "        '''\n",
    "        TreeGenerate函数用于递归构建决策树，伪代码参照课件中的“Algorithm 1 决策树学习基本算法”\n",
    "            参数：\n",
    "            - D: 数据集，二维数组。数据的前 D.shape[1]-1 列为特征，最后一列为对应的类别标签。\n",
    "            - A: 可用的特征集合（一个列表），用于指导决策树的划分。当 A 为空或为 None 时，表示没有更多可用特征进行划分。\n",
    "                 例如，A=[0,1]代表第0、第1个特征\n",
    "                 注意不是特征的取值，而是特征的索引\n",
    "\n",
    "            返回值：\n",
    "            - 一个决策树节点（Node 对象），该节点可能为叶节点（记录类别标签）或内部节点（记录划分特征及其分支）。\n",
    "        '''    \n",
    "        train_f = D[:, :D.shape[1] - 1]# 提取特征\n",
    "        train_o = D[:, D.shape[1] - 1: D.shape[1]]# 提取标签\n",
    "        \n",
    "        # 生成结点 node   \n",
    "        node = Node()\n",
    "\n",
    "        # 终止条件1：\n",
    "        # if D中样本全属于同一类别C then\n",
    "        #     将node标记为C类叶结点并返回\n",
    "        # end if\n",
    "        # =============================\n",
    "        # Tips: 可能用到的函数\n",
    "        # 1. np.unique(): 用于统计数据集D中标签的非重复值，如果只有一个非重复值，那么D中样本全部属于同一类别\n",
    "        # 2. Node(isLeaf=True, label=..., feature_index=-1): 构造一个叶节点，label参数传入当前样本的类别C\n",
    "        # -------Your Code Here-------\n",
    "       \n",
    "        \n",
    "        # 终止条件2：\n",
    "        # if A = Ø （无可用特征）OR D中样本在A上取值相同 then\n",
    "        #     将node标记叶结点，其类别标记为D中样本数最多的类并返回 \n",
    "        # end if\n",
    "        # =============================\n",
    "        # Tips: 可能用到的函数和语法：\n",
    "        # 1. if A == None 或 bool(A) == False: 判断A是否为空\n",
    "        # 2. np.unique(train_f[:, d]): 用于统计D中样本在特征集A中的某个特征d的非重复值，\n",
    "        #                              如果只有一个非重复值，说明D中样本在特征d上取值相同\n",
    "        #                              遍历A中所有特征，如果D中样本在A中所有特征上都只有一个非重复值，那么D中样本在A上取值相同\n",
    "        # 3. np.argmax(np.bincount(train_o[:, 0])): 找到 train_o（类别标签数组）中出现次数最多的类别（即众数）\n",
    "        # 4. node = Node(isLeaf=True, label=np.argmax(np.bincount(train_o[:, 0])), feature_index=-1)\n",
    "        #    将node标记叶结点，其类别标记为D中样本数最多的类\n",
    "        # -------Your Code Here-------\n",
    "\n",
    "\n",
    "        # 开始构造非叶节点\n",
    "        # 首先统计每个特征可能的取值，存储于决策树的possible_value属性中\n",
    "        for d in A:\n",
    "            self.possible_value[d] = np.unique(D[:, d])\n",
    "        node.isLeaf = False\n",
    "        \n",
    "        # 从A中选择最优划分特征a_star\n",
    "        # （选择信息增益最大的特征，用到上面实现的find_opt_feature_gain函数） \n",
    "        # -------Your Code Here-------\n",
    "        _,a_star = \n",
    "        node.feature_index = a_star\n",
    "        \n",
    "        # 复制 A 并移除已选择的划分特征\n",
    "        Av = A.copy()\n",
    "        Av.remove(a_star)\n",
    "        \n",
    "        # for a_star 的每一个值a_star_v do\n",
    "        #     为node 生成每一个分支node_a_star_v；令D_v表示D中在a_star上取值为a_star_v的样本子集\n",
    "        #     if D_v 为空 then\n",
    "        #         将分支结点标记为叶结点，其类别标记为D中样本最多的类\n",
    "        #     else\n",
    "        #         递归调用TreeGenerate(D_v,A-{a_star}) 创建子树\n",
    "        #     end if\n",
    "        #     将子节点加入树\n",
    "        # end for\n",
    "        # =================================\n",
    "        # Tips: 可能用到的函数或语法\n",
    "        # 1. self.possible_value[a_star]或者np.unique(D[:, a_star]): 获得a_star特征的非重复值，用于遍历\n",
    "        # 2. Dv = D[np.where(D[:, a_star] == a_star_v)]表示D中在a_star上取值为a_star_v的样本子集:\n",
    "        #    - np.where() 接受一个布尔数组作为条件，返回满足条件（即布尔值为 True）的元素的索引。\n",
    "        #    - np.where(D[:, a_star] == a_star_v) 会返回一个索引数组，\n",
    "        #      这些索引指明了 D 数组第 a_star 列中所有等于 a_star_v 的元素所在的行号。\n",
    "        # 3. np.size(Dv): 如果样本子集Dv的大小为0，则Dv为空集\n",
    "        # 4. Node(isLeaf=True, label=np.argmax(np.bincount(D_o[:,0])), feature_index=-1): 将分支结点标记为叶结点，其类别标记为D中样本最多的类\n",
    "        # 5. node.addNode(val=a_star_v, node=node_a_star_v): 将子节点加入树\n",
    "        # -------Your Code Here-------\n",
    "            \n",
    "\n",
    "        # 返回当前节点\n",
    "        return node\n",
    "\n",
    " \n",
    "    def train(self, D):\n",
    "        '''\n",
    "        train函数调用TreeGenerate函数来递归地生成决策树\n",
    "    \n",
    "        '''\n",
    "        D = np.array(D) # 将Dataframe对象转换为numpy矩阵（也可以不转，自行决定做法）\n",
    "        A = set(range(D.shape[1] - 1)) # 特征集A\n",
    "        \n",
    "        #记下每个特征可能的取值\n",
    "        for every in A:\n",
    "            self.possible_value[every] = np.unique(D[:, every])\n",
    "        \n",
    "        # 递归地生成决策树，并将决策树的根结点赋值给self.tree_root\n",
    "        # 使用上面实现的TreeGenerate函数\n",
    "        # -------Your Code Here-------\n",
    "        self.tree_root = \n",
    "        return\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict(self, D):\n",
    "        '''\n",
    "        predict函数对测试集D进行预测， 并输出预测准确率（预测正确的个数 / 总数据数量）\n",
    "        \n",
    "        '''\n",
    "        D = np.array(D) # 将Dataframe对象转换为numpy矩阵（也可以不转，自行决定做法）\n",
    "\n",
    "        # 实现思路：\n",
    "        # 1. 初始化变量：确定样本数量 n，创建一个与样本数量匹配的 labels 数组存储每个样本的预测标签，以及用 correct 计数预测正确的样本数；\n",
    "        # 2. 对每一个样本i in range(n)：\n",
    "        #    a. 从决策树的根结点开始（即 self.tree_root），记当前结点x=self.tree_root；\n",
    "        #    b. 利用 while 循环不断判断当前结点x是否为分支节点（x.isLeaf == False），\n",
    "        #       如果是，则根据样本i在当前结点划分特征（x.feature_index）上的取值，从结点的 children 字典中选出对应的子结点（即x=x.children[d[x.index]]，其中d为当前的样本i）；\n",
    "        #    c. 当遍历到叶节点后，该叶结点的标签就是样本i的预测标签，将该叶节点上的 label 存入预测标签数组；\n",
    "        #    d. 同时，将预测标签与真实标签（样本i的最后一列 D[i, D.shape[1]-1]）进行比较，统计预测正确的个数；\n",
    "        # 3. 计算并返回预测准确率\n",
    "        # -------Your Code Here-------\n",
    "        \n",
    "        return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建决策树\n",
    "train_frame = pd.read_csv('train_titanic.csv')\n",
    "dt = DTree()\n",
    "dt.train(train_frame)\n",
    "\n",
    "# 利用构建好的决策树对测试数据集进行预测，输出预测准确率（预测正确的个数 / 总数据数量）\n",
    "test_frame = pd.read_csv('test_titanic.csv')\n",
    "acc=dt.predict(test_frame)\n",
    "print(\"预测准确率:\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#展示生成的决策树结构\n",
    "def display_tree(node, indent=''):\n",
    "    if node.isLeaf:\n",
    "        print(indent + \"Leaf Node: label =\", node.label)\n",
    "    else:\n",
    "        print(indent + \"Branch Node: feature_index =\", node.feature_index)\n",
    "        for value, child in node.children.items():\n",
    "            print(indent + \"|--> Child Value:\", value)\n",
    "            display_tree(child, indent + \"   \")\n",
    "\n",
    "display_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color = blue size=4>第三部分:作业提交</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一、本次实验分两周完成，实验课下课前提交完成代码 要求:\n",
    "\n",
    "1)文件格式为：学号-姓名.ipynb\n",
    "\n",
    "2)【不要】提交文件夹、压缩包、数据集等无关文件，只需提交单个ipynb文件即可，如果交错请到讲台前联系助教，删掉之前的错误版本后再进行提交  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二、实验课件获取地址:https://www.jianguoyun.com/p/DUImtU8Qp5WhChixu4sGIAA\n",
    "\n",
    "课堂课件获取地址:https://www.jianguoyun.com/p/Ddk4kPQQp5WhChiXwYcGIAA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
